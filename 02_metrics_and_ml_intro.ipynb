{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics + Intro to training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in classification\n",
    "\n",
    "The most common metrics for classification problems are:\n",
    "- **Accuracy**: the proportion of correct predictions\n",
    "- **Precision**: the proportion of true positive predictions among all positive predictions (popular: the ability to precisely identify instances of a class)\n",
    "- **Recall**: the proportion of true positive predictions among all actual positive instances (popular: the ability to find all instances of a class)\n",
    "- **F1 score**: the harmonic mean of precision and recall (popular: the best of both worlds)\n",
    "- **Macro and micro**: terms used to specify how to calculate precision and recall for multiclass classification problems. Macro precision is the average of precision for each class, while micro precision is the proportion of true positive predictions among all positive predictions, regardless of class.\n",
    "- **Balanced accuracy**: the same as macro average recall\n",
    "- **Confusion matrix**: a table showing the number of true positive, true negative, false positive, and false negative predictions\n",
    "- **Classification report**: a summary of precision, recall, F1 score, and support for each class\n",
    "\n",
    "Metrics in scikit-learn: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n",
    "| **Term**               | **Description**                                                                                       |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| **True Positive (TP)** | The model correctly predicted the positive class. Example: The model predicted \"yes\" and the actual label was also \"yes\".   |\n",
    "| **False Positive (FP)** | The model incorrectly predicted the positive class. Example: The model predicted \"yes\" but the actual label was \"no\". This is also called a **Type I error**.   |\n",
    "| **True Negative (TN)** | The model correctly predicted the negative class. Example: The model predicted \"no\" and the actual label was also \"no\".     |\n",
    "| **False Negative (FN)** | The model incorrectly predicted the negative class. Example: The model predicted \"no\" but the actual label was \"yes\". This is also called a **Type II error**.    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification:\n",
    "\n",
    "The scikit-learn convention and more normal in classic statistics is to have TN in the top left corner.\n",
    "\n",
    "|                          | **Predicted Negative** | **Predicted Positive** |\n",
    "|--------------------------|------------------------|------------------------|\n",
    "| **Actual Negative**      | True Negative (TN)     | False Positive (FP)    |\n",
    "| **Actual Positive**      | False Negative (FN)    | True Positive (TP)     |\n",
    "\n",
    "If you focus on the True Positive you will quite often see the confusion matrix written as:\n",
    "\n",
    "|                   | **Predicted Positive** | **Predicted Negative** |\n",
    "|-------------------|------------------------|------------------------|\n",
    "| **Actual Positive**   | True Positive (TP)      | False Negative (FN)     |\n",
    "| **Actual Negative**   | False Positive (FP)     | True Negative (TN)      |\n",
    "\n",
    "\n",
    "Here we have presented the confusion matrix with actual (true) values in rows and predicted values in columns. This is the most common way of presenting the confusion matrix in machine learning. However, quite often you will see the predicted values presented in the rows and the actual values in the columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass:\n",
    "\n",
    "In multiclass it is standard to have the TP for each class in the diagonal.\n",
    "\n",
    "|                  | Predicted Class 0 | Predicted Class 1 | Predicted Class 2 |\n",
    "|------------------|-------------------|-------------------|-------------------|\n",
    "| **Actual Class 0** | TP (Class 0)       | FN (Class 0 as Class 1) | FN (Class 0 as Class 2) |\n",
    "| **Actual Class 1** | FP (Class 1 as Class 0) | TP (Class 1)       | FN (Class 1 as Class 2) |\n",
    "| **Actual Class 2** | FP (Class 2 as Class 0) | FP (Class 2 as Class 1) | TP (Class 2)       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: Be aware of the different ways of presenting the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "### Recall\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### F1 Score\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "### Accuracy\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - a binary classification of breast cancer\n",
    "\n",
    "The goal is to detect breast cancer (malignant or benign) based on features such as the mean radius, mean texture, and mean perimeter of the cell nuclei. The dataset is available in scikit-learn. The negative class (0) is malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Load the Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target  # type: ignore\n",
    "\n",
    "# NOTE: dataset returned from scikit learn is in numpy array format, not pandas dataframe format\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "# Parameters set in train_test_split:\n",
    "# stratify=y: This is used to ensure that the training and testing sets have approximately the same percentage of samples of each target class as the complete set.\n",
    "# shuffle=True: This is used to shuffle the data before splitting it, making sure that the data is not ordered in any way when creating the training and testing sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspects the numpy format of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspects the first 3 rows of the training set\n",
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspects the first 10 values of the target variable in the training set\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the label numbers in each class. For numpy arrays we cannot use the classic value_counts() method we use for a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Random Forest model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspects the first 10 predictions\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualise the confusion matrix in the standard scikit-learn way\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=conf_matrix, display_labels=cancer.target_names\n",
    ")\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix (Standard scikit-learn Layout)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix (standard layout in scikit-learn):\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        conf_matrix, columns=[\"Malignant\", \"Benign\"], index=[\"Malignant\", \"Benign\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Calculate and display accuracy, precision, and recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate the metrics manually or use the classification_report function in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=cancer.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap - Binary confusion matrix in Scikit-learn\n",
    "\n",
    "\n",
    "|                          | **Predicted Negative** | **Predicted Positive** |\n",
    "|--------------------------|------------------------|------------------------|\n",
    "| **Actual Negative**      | True Negative (TN)     | False Positive (FP)    |\n",
    "| **Actual Positive**      | False Negative (FN)    | True Positive (TP)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual calculations of metrics for binary classification to see what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: TP + TN / (TP + TN + FP + FN)\n",
    "(102 + 58) / (102 + 58 + 6 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision of positive class: TP / (TP + FP)\n",
    "102 / (102 + 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall of positive class: TP / (TP + FN)\n",
    "102 / (102 + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - classifying iris flowers - multiclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Load the Iris dataset: https://archive.ics.uci.edu/dataset/53/iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target  # type: ignore\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Random Forest model\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "clf = LogisticRegression(random_state=42, max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(conf_matrix, columns=iris.target_names, index=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Calculate and display accuracy, precision, and recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report for comparison\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Compute confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualise the confusion matrix in the standard scikit-learn way\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=conf_matrix, display_labels=iris.target_names\n",
    ")\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix (Standard scikit-learn Layout)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course-machine-learning-for-geotechnics-intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
