# Copilot Instructions for CPT-to-Soiltype ML Project

## Project Overview
Educational ML course for Norwegian Geotechnical Institute (NGI) that predicts soil types (1-7: gravel to clay) from Cone Penetration Test (CPT) data. Uses Oberhollenzer CPT dataset for teaching ML fundamentals in geotechnics.

## Project Structure
- **`notebooks/`**: Primary workflow - Jupyter notebooks for hands-on learning
  - `setup.ipynb`: Download and extract the Oberhollenzer dataset (run once)
  - `eda_cpt.ipynb`: Data exploration and profiling with ydata-profiling
  - `metrics_and_ml_intro.ipynb`: ML fundamentals and evaluation metrics
  - `training_evaluation.ipynb`: Model training with sklearn, imblearn pipelines
- **`data/raw/`**: Oberhollenzer dataset (`CPT_PremstallerGeotechnik_revised.csv`)
- **`data/model_ready/`**: Preprocessed train/test splits (generated by notebooks)

## Development Environment

### Setup
```bash
uv sync                    # Install dependencies (creates .venv)
```

### Dependencies
Core ML stack managed via `uv` in `pyproject.toml`:
- **pandas**: Data manipulation and CSV I/O
- **scikit-learn**: ML algorithms (KNN, Random Forest, etc.) and preprocessing
- **imbalanced-learn**: SMOTE oversampling and RandomUnderSampler
- **xgboost**: Gradient boosting (optional, for advanced models)
- **pyod**: Outlier detection (MAD, IForest)
- **ydata-profiling**: Automated EDA reports
- **matplotlib**: Plotting and visualization
- **ipykernel**: Jupyter kernel for running notebooks in VS Code
- **jupyter**: Jupyter notebook environment
- **ruff**: Code formatting and linting

### Running Notebooks
- Execute cells sequentially - notebooks build on each other
- Standard Python ≥3.12 required (see `.python-version`)
- First run `eda_cpt.ipynb` to generate preprocessed data

## Critical Patterns

### Standard CPT Features
All notebooks use this exact feature list (note special characters in names):
```python
FEATURES = [
    "Depth (m)", "qc (MPa)", "fs (kPa)", "Rf (%)",
    "σ,v (kPa)", "u0 (kPa)", "σ',v (kPa)",
    "Qtn (-)", "Fr (%)"
]
LABELS = "Oberhollenzer_classes"  # Target variable (1-7)
```

### Imbalanced Data Handling (Required Pattern)
Use imblearn pipeline with sequential resampling:
```python
from imblearn.pipeline import make_pipeline
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

# 1. Undersample majority classes → 110k samples
# 2. Oversample minority classes → 75k samples
pipeline = make_pipeline(
    StandardScaler(),
    RandomUnderSampler(sampling_strategy={...}, random_state=42),
    SMOTE(sampling_strategy={...}, random_state=42),
    KNeighborsClassifier(n_neighbors=5)
)
```

### File Path Convention
Notebooks reference data using relative paths from `notebooks/` directory:
```python
FILEPATH_TRAIN = "../data/model_ready/dataset_train.csv"
FILEPATH_TEST = "../data/model_ready/dataset_test.csv"
```

### Soil Type Classification
Dataset uses 7-class schema (Oberhollenzer):
1. Gravel
2. Fine grained organic soils
3. Coarse grained organic soils  
4. Sand to gravel
5. Sand
6. Silt to fine sand
7. Clay to silt

## Common Workflows

### Data Preparation (Required First Step)
1. Open `setup.ipynb` and run all cells to download and extract the dataset
2. Open `eda_cpt.ipynb` and run all cells sequentially
3. Loads raw CSV from `data/raw/CPT_PremstallerGeotechnik_revised.csv`
4. Performs data cleaning:
   - Drops duplicates and null values
   - Removes hardcoded outliers (qc > 0, u0 ≥ 0, etc.)
   - Applies univariate outlier detection (MAD) and multivariate (IForest)
5. Splits data by unique IDs (prevents data leakage across boreholes)
6. **Saves preprocessed datasets** to `data/model_ready/`:
   - `dataset_train.csv`: Training set (75% of data)
   - `dataset_test.csv`: Test set (25% of data)
7. Generates ydata-profiling reports for EDA

### Model Training
1. Ensure `data/model_ready/` contains train/test CSVs (run `eda_cpt.ipynb` first)
2. Open `training_evaluation.ipynb`
3. Start with DummyClassifier baseline (always compare against this)
4. Use imblearn pipelines for proper data leakage prevention
5. Evaluate with sklearn `classification_report(y_test, y_pred, zero_division=0)`

### Code Formatting
```bash
uv run ruff format .          # Auto-format code
uv run ruff check --fix .     # Lint and auto-fix issues
```

### Debugging Data Issues
- Feature names contain Unicode characters (σ, ') - use exact column names
- Class 3.0 is sometimes filtered: `df_train[df_train[LABELS] != 3.0]`
- Always check class distribution with `y_train.value_counts()` before/after resampling

## Educational Context
- Notebooks demonstrate concepts step-by-step (no production-ready scripts exist yet)
- Emphasis on understanding ML fundamentals over performance optimization  
- Dummy models establish performance baselines for scientific papers